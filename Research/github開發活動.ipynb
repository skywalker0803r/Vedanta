{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce581bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 01:02:21,416 - INFO - å‰µå»ºæ–°çš„ CSV æª”æ¡ˆ: github_growth_rank.csv\n",
      "2025-11-15 01:02:21,434 - INFO - å¾ github_growth_rank.csv è¼‰å…¥ 0 å€‹å·²è™•ç†ç¬¦è™Ÿã€‚\n",
      "2025-11-15 01:02:21,638 - INFO - æ‰¾åˆ° 100 å€‹ç†±é–€å¹£ç¨®ç¬¦è™Ÿ å‰äº”ç‚º:['btc', 'eth', 'usdc', 'sol', 'zec']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†å¹£ç¨®:   0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 01:02:21,691 - INFO - btc ä¸å­˜åœ¨csvä¸­ é–‹å§‹è·‘åˆ†æ å˜—è©¦å–å¾—CoinGecko ID\n",
      "2025-11-15 01:02:21,693 - INFO - ä½¿ç”¨get_coingecko_idå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:02:33,962 - INFO - æ‰¾åˆ° btc çš„ CoinGecko ID:bitcoin\n",
      "2025-11-15 01:02:33,965 - INFO - ä½¿ç”¨fetch_coingecko_github_repo_urlå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:02:46,511 - INFO - æ‰¾åˆ° bitcoin çš„ GitHub repo é€£çµ:https://github.com/bitcoin/bitcoin\n",
      "2025-11-15 01:02:46,513 - INFO - âœ… æˆåŠŸå¾ URL æå– owner:bitcoin, repo:bitcoin from https://github.com/bitcoin/bitcoin\n",
      "2025-11-15 01:02:46,515 - INFO - å˜—è©¦ç²å–æäº¤æ´»å‹•: https://api.github.com/repos/bitcoin/bitcoin/stats/commit_activity å»¶é²10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:03:01,769 - INFO - å˜—è©¦ç²å–å„²å­˜åº«è³‡è¨Š: https://api.github.com/repos/bitcoin/bitcoin\n",
      "2025-11-15 01:03:02,272 - INFO - æ‹¿å– bitcoin çš„ commits, stars, forks æˆåŠŸ\n",
      "2025-11-15 01:03:02,275 - INFO - è¨ˆç®— growth æˆåŠŸ coingecko_id:bitcoin\n",
      "2025-11-15 01:03:05,238 - INFO - âœ… æˆåŠŸå°‡ btc çš„æ•¸æ“šå¯«å…¥ github_growth_rank.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†å¹£ç¨®:   1%|          | 1/100 [00:43<1:11:51, 43.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 01:03:05,244 - INFO - eth ä¸å­˜åœ¨csvä¸­ é–‹å§‹è·‘åˆ†æ å˜—è©¦å–å¾—CoinGecko ID\n",
      "2025-11-15 01:03:05,248 - INFO - ä½¿ç”¨get_coingecko_idå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:03:18,592 - INFO - æ‰¾åˆ° eth çš„ CoinGecko ID:ethereum\n",
      "2025-11-15 01:03:18,595 - INFO - ä½¿ç”¨fetch_coingecko_github_repo_urlå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:03:29,534 - INFO - æ‰¾åˆ° ethereum çš„ GitHub repo é€£çµ:https://github.com/ethereum/go-ethereum\n",
      "2025-11-15 01:03:29,535 - INFO - âœ… æˆåŠŸå¾ URL æå– owner:ethereum, repo:go-ethereum from https://github.com/ethereum/go-ethereum\n",
      "2025-11-15 01:03:29,536 - INFO - å˜—è©¦ç²å–æäº¤æ´»å‹•: https://api.github.com/repos/ethereum/go-ethereum/stats/commit_activity å»¶é²10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:03:41,576 - INFO - å˜—è©¦ç²å–å„²å­˜åº«è³‡è¨Š: https://api.github.com/repos/ethereum/go-ethereum\n",
      "2025-11-15 01:03:42,087 - INFO - æ‹¿å– ethereum çš„ commits, stars, forks æˆåŠŸ\n",
      "2025-11-15 01:03:42,090 - INFO - è¨ˆç®— growth æˆåŠŸ coingecko_id:ethereum\n",
      "2025-11-15 01:03:44,418 - INFO - âœ… æˆåŠŸå°‡ eth çš„æ•¸æ“šå¯«å…¥ github_growth_rank.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "è™•ç†å¹£ç¨®:   2%|â–         | 2/100 [01:22<1:06:55, 40.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-11-15 01:03:44,421 - INFO - usdc ä¸å­˜åœ¨csvä¸­ é–‹å§‹è·‘åˆ†æ å˜—è©¦å–å¾—CoinGecko ID\n",
      "2025-11-15 01:03:44,422 - INFO - ä½¿ç”¨get_coingecko_idå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:03:59,411 - INFO - æ‰¾åˆ° usdc çš„ CoinGecko ID:usd-coin\n",
      "2025-11-15 01:03:59,413 - INFO - ä½¿ç”¨fetch_coingecko_github_repo_urlå…ˆç­‰10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:04:12,977 - INFO - æ‰¾åˆ° usd-coin çš„ GitHub repo é€£çµ:https://github.com/centrehq/centre-tokens\n",
      "2025-11-15 01:04:12,978 - INFO - âœ… æˆåŠŸå¾ URL æå– owner:centrehq, repo:centre-tokens from https://github.com/centrehq/centre-tokens\n",
      "2025-11-15 01:04:12,979 - INFO - å˜—è©¦ç²å–æäº¤æ´»å‹•: https://api.github.com/repos/centrehq/centre-tokens/stats/commit_activity å»¶é²10ç§’é¿å…é™æµ\n",
      "2025-11-15 01:04:24,396 - INFO - â³ GitHub æ•¸æ“šæ­£åœ¨è¨ˆç®—ä¸­ (202 Accepted)ï¼Œç­‰å¾… 6.74 ç§’å¾Œé‡è©¦... (ç¬¬ 1 æ¬¡é‡è©¦)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "import sys\n",
    "import random\n",
    "import logging\n",
    "from urllib.parse import urlparse\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "load_dotenv()\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"log.txt\", encoding='utf-8'),\n",
    "                        logging.StreamHandler(sys.stdout)\n",
    "                    ])\n",
    "\n",
    "# å¾ç’°å¢ƒè®Šæ•¸ç²å– API Keys\n",
    "GITHUB_TOKEN = os.getenv('GITHUB_TOKEN')\n",
    "GOOGLE_API_KEY = os.getenv('GOOGLE_API_KEY')\n",
    "GOOGLE_CSE_ID = os.getenv('GOOGLE_CSE_ID')\n",
    "\n",
    "if not GITHUB_TOKEN:\n",
    "    logging.error(\"GITHUB_TOKEN ç’°å¢ƒè®Šæ•¸æœªè¨­å®šã€‚è«‹æª¢æŸ¥æ‚¨çš„ .env æ–‡ä»¶ã€‚\")\n",
    "    sys.exit(1)\n",
    "if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:\n",
    "    logging.warning(\"GOOGLE_API_KEY æˆ– GOOGLE_CSE_ID ç’°å¢ƒè®Šæ•¸æœªè¨­å®šã€‚Google æœå°‹å‚™ç”¨æ–¹æ¡ˆå°‡ç„¡æ³•ä½¿ç”¨ã€‚\")\n",
    "\n",
    "GITHUB_HEADERS = {\n",
    "    'Accept': 'application/vnd.github+json',\n",
    "    'Authorization': f'token {GITHUB_TOKEN}'\n",
    "}\n",
    "\n",
    "SUCCESS_FILE = 'success_symbols.csv'\n",
    "CSV_FILE = 'github_growth_rank.csv'\n",
    "\n",
    "def get_top_symbols(limit=100, quote_asset='USDT'):\n",
    "    \"\"\"å¾å¹£å®‰ API ç²å–å¸‚å€¼å‰ N åçš„å¹£ç¨®ç¬¦è™Ÿã€‚\"\"\"\n",
    "    url = \"https://api.binance.com/api/v3/ticker/24hr\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status() # æª¢æŸ¥ HTTP éŒ¯èª¤\n",
    "        data = response.json()\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        logging.error(f\"å¾å¹£å®‰ API ç²å–æ•¸æ“šå¤±æ•—: {e}\")\n",
    "        return []\n",
    "\n",
    "    usdt_pairs = [item for item in data if item['symbol'].endswith(quote_asset)]\n",
    "    sorted_pairs = sorted(usdt_pairs, key=lambda x: float(x.get('quoteVolume', 0)), reverse=True)\n",
    "    top_symbols = [item['symbol'][:-len(quote_asset)].lower() for item in sorted_pairs[:limit]]\n",
    "    return top_symbols\n",
    "\n",
    "# ä¿®æ”¹é€™å‡½æ•¸ å¦‚æœerroræ˜¯202æˆ–429 å°±æ˜¯èªªåªéœ€ç­‰å¾…å†é‡è©¦å°±èƒ½è·‘çš„å°±é‡è©¦ è‹¥æ˜¯404é‚£ç¨®å°±ç›´æ¥return None\n",
    "def fetch_with_retry(url, headers=None, max_retries=99999, initial_delay=5, backoff_factor=2, return_headers=False):\n",
    "    \"\"\"\n",
    "    å¸¶æœ‰é‡è©¦æ©Ÿåˆ¶çš„ HTTP è«‹æ±‚å‡½æ•¸ã€‚\n",
    "    å°æ–¼ 202 å’Œ 429 ç‹€æ…‹ç¢¼ï¼Œæœƒé€²è¡Œé‡è©¦ã€‚\n",
    "    å°æ–¼ 404 åŠå…¶ä»– 4xx ç‹€æ…‹ç¢¼ï¼Œæœƒç›´æ¥è¿”å› Noneã€‚\n",
    "    æ–°å¢ return_headers åƒæ•¸ï¼Œå¦‚æœç‚º Trueï¼Œå‰‡è¿”å› response.headersã€‚\n",
    "    \"\"\"\n",
    "    for i in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, timeout=15)\n",
    "\n",
    "            if response.status_code == 202:  # GitHub API æ•¸æ“šæ­£åœ¨è¨ˆç®—ä¸­\n",
    "                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)\n",
    "                logging.info(f\"â³ GitHub æ•¸æ“šæ­£åœ¨è¨ˆç®—ä¸­ (202 Accepted)ï¼Œç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦... (ç¬¬ {i+1} æ¬¡é‡è©¦)\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            elif response.status_code == 429:  # è«‹æ±‚éæ–¼é »ç¹\n",
    "                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 5) # 429 å¯ä»¥çµ¦æ›´é•·çš„å»¶é²\n",
    "                logging.warning(f\"Too Many Requests (429)ï¼Œç­‰å¾… {delay:.2f} ç§’å¾Œé‡è©¦... (ç¬¬ {i+1} æ¬¡é‡è©¦)\")\n",
    "                time.sleep(delay)\n",
    "                continue\n",
    "            elif 400 <= response.status_code < 500: # 4xx å®¢æˆ¶ç«¯éŒ¯èª¤ï¼Œé™¤äº† 429\n",
    "                if response.status_code == 404:\n",
    "                    logging.error(f\"è«‹æ±‚ {url} å¤±æ•—: è³‡æºæœªæ‰¾åˆ° (404 Not Found)ã€‚ä¸å†é‡è©¦ã€‚\")\n",
    "                else:\n",
    "                    logging.error(f\"è«‹æ±‚ {url} å¤±æ•—: å®¢æˆ¶ç«¯éŒ¯èª¤ {response.status_code}ã€‚ä¸å†é‡è©¦ã€‚\")\n",
    "                return None # å°æ–¼ 4xx éŒ¯èª¤ï¼Œç›´æ¥è¿”å› None\n",
    "\n",
    "            response.raise_for_status()  # å°æ–¼é 2xx ç‹€æ…‹ç¢¼ï¼ˆé€™è£¡ä¸»è¦æ˜¯ 5xx éŒ¯èª¤ï¼‰ï¼Œæ‹‹å‡º HTTPError\n",
    "\n",
    "            # æ–°å¢é‚è¼¯ï¼šå¦‚æœ return_headers ç‚º Trueï¼Œå‰‡è¿”å› response.headers\n",
    "            if return_headers:\n",
    "                return response.headers\n",
    "\n",
    "            try:\n",
    "                return response.json()\n",
    "            except json.JSONDecodeError as e:\n",
    "                logging.error(f\"è§£æ JSON å¤±æ•—: {e}, URL: {url}, éŸ¿æ‡‰å…§å®¹: {response.text}\")\n",
    "                if i < max_retries - 1:\n",
    "                    delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)\n",
    "                    logging.warning(f\"JSON è§£æå¤±æ•—ï¼Œé‡è©¦ {url} (ç¬¬ {i+1} æ¬¡), ç­‰å¾… {delay:.2f} ç§’...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    logging.error(f\"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è«‹æ±‚ {url} (JSON è§£æå¤±æ•—)ã€‚\")\n",
    "                    return None\n",
    "\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"è«‹æ±‚ {url} å¤±æ•—: {e}\")\n",
    "            if i < max_retries - 1:\n",
    "                delay = initial_delay * (backoff_factor ** i) + random.uniform(0, 2)\n",
    "                logging.warning(f\"é‡è©¦ {url} (ç¬¬ {i+1} æ¬¡), ç­‰å¾… {delay:.2f} ç§’...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                logging.error(f\"é”åˆ°æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œæ”¾æ£„è«‹æ±‚ {url}ã€‚\")\n",
    "    return None\n",
    "\n",
    "def get_coingecko_id(symbol):\n",
    "    \"\"\"æ ¹æ“šç¬¦è™Ÿç²å– CoinGecko IDã€‚\"\"\"\n",
    "    logging.info(\"ä½¿ç”¨get_coingecko_idå…ˆç­‰10ç§’é¿å…é™æµ\")\n",
    "    time.sleep(random.uniform(10, 15))\n",
    "    url = f\"https://api.coingecko.com/api/v3/search?query={symbol}\"\n",
    "    # ä¿®æ”¹é»ï¼šä½¿ç”¨ fetch_with_retry\n",
    "    data = fetch_with_retry(url) # fetch_with_retry æœƒè¿”å› None æˆ– JSON æ•¸æ“š\n",
    "    if data and data.get('coins'): # ä½¿ç”¨ .get() é¿å… KeyError\n",
    "        # å˜—è©¦æ‰¾åˆ°ç²¾ç¢ºåŒ¹é…çš„symbolæˆ–id\n",
    "        for coin in data['coins']:\n",
    "            if coin['symbol'].lower() == symbol.lower() or coin['id'].lower() == symbol.lower():\n",
    "                return coin['id']\n",
    "        # å¦‚æœæ²’æœ‰ç²¾ç¢ºåŒ¹é…ï¼Œè¿”å›ç¬¬ä¸€å€‹çµæœ\n",
    "        return data['coins'][0]['id']\n",
    "    logging.error(f\"å¾ CoinGecko æœå°‹ ID å¤±æ•—æˆ–ç„¡çµæœ: {symbol}\")\n",
    "    return None\n",
    "\n",
    "def fetch_coingecko_github_repo_url(coingecko_id):\n",
    "    \"\"\"å¾ CoinGecko API ç²å– GitHub å„²å­˜åº«é€£çµã€‚\"\"\"\n",
    "    logging.info(\"ä½¿ç”¨fetch_coingecko_github_repo_urlå…ˆç­‰10ç§’é¿å…é™æµ\")\n",
    "    time.sleep(random.uniform(10, 15))\n",
    "    url = f\"https://api.coingecko.com/api/v3/coins/{coingecko_id}\"\n",
    "    # ä¿®æ”¹é»ï¼šä½¿ç”¨ fetch_with_retry\n",
    "    data = fetch_with_retry(url)\n",
    "    if data and 'links' in data and 'repos_url' in data['links'] and data['links']['repos_url']:\n",
    "        # å¯èƒ½æœ‰å…©ç¨®è·¯å¾‘\n",
    "        github_urls = [link for link in data['links']['repos_url'][\"github\"] if link and \"github.com\" in link]\n",
    "        if github_urls:\n",
    "            return github_urls[0] # è¿”å›ç¬¬ä¸€å€‹ GitHub é€£çµ\n",
    "        else:\n",
    "            github_urls = [link for link in data['links']['repos_url'] if link and \"github.com\" in link]\n",
    "            if github_urls:\n",
    "                return github_urls[0] # è¿”å›ç¬¬ä¸€å€‹ GitHub é€£çµ\n",
    "    logging.warning(f\"âŒ CoinGecko æ‰¾ä¸åˆ° {coingecko_id} çš„ GitHub repo é€£çµã€‚URL: {url}\")\n",
    "    return None\n",
    "\n",
    "def Google_Search_github_repo(query, limit=5):\n",
    "    \"\"\"ä½¿ç”¨ Google Custom Search API æœå°‹ GitHub å„²å­˜åº«é€£çµã€‚\"\"\"\n",
    "    if not GOOGLE_API_KEY or not GOOGLE_CSE_ID:\n",
    "        logging.warning(\"Google API Keys æœªè¨­å®šï¼Œè·³é Google æœå°‹ã€‚\")\n",
    "        return None\n",
    "\n",
    "    search_url = f\"https://www.googleapis.com/customsearch/v1?key={GOOGLE_API_KEY}&cx={GOOGLE_CSE_ID}&q={query}&num={limit}\"\n",
    "    logging.info(\"é¿å…è¢«googleé™æµ å»¶é²10ç§’å†ç”¨googleæœå°‹\")\n",
    "    time.sleep(10) # Keep the delay as it's a specific requirement for Google Search\n",
    "\n",
    "    # Use fetch_with_retry for the Google Custom Search API call\n",
    "    data = fetch_with_retry(search_url)\n",
    "\n",
    "    if data and 'items' in data:\n",
    "        for item in data['items']:\n",
    "            link = item.get('link')\n",
    "            if link and \"github.com\" in link and \"/tree/\" not in link and \"/blob/\" not in link and \"/wiki/\" not in link and \"/topics/\" not in link:\n",
    "                # å˜—è©¦éæ¿¾æ‰é repo æ ¹ç›®éŒ„çš„é€£çµ\n",
    "                # ä¸¦ä¸”ç¢ºä¿ä¸æ˜¯ Gist æˆ– Pages é€™ç¨®\n",
    "                parsed_url = urlparse(link)\n",
    "                path_parts = [p for p in parsed_url.path.split('/') if p]\n",
    "                if len(path_parts) >= 2: # è‡³å°‘æœ‰ owner/repo\n",
    "                    logging.info(f\"âœ… Google æœå°‹æ‰¾åˆ° GitHub é€£çµ: {link} (Query: {query})\")\n",
    "                    return link\n",
    "        logging.warning(f\"ğŸ” Google æœå°‹æœªæ‰¾åˆ°ç›¸é—œçš„ GitHub å„²å­˜åº«é€£çµ (Query: {query})\")\n",
    "    else:\n",
    "        logging.info(f\"ğŸ” Google æœå°‹ç„¡çµæœ (Query: {query})\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_github_owner_repo(repo_url):\n",
    "    \"\"\"å¾ GitHub URL ä¸­æå– owner å’Œ repository åç¨±ï¼Œä¸¦è™•ç†çµ„ç¹” URLã€‚\"\"\"\n",
    "    if not repo_url:\n",
    "        return None, None\n",
    "    parsed_url = urlparse(repo_url)\n",
    "    path_parts = [p for p in parsed_url.path.split('/') if p]\n",
    "\n",
    "    if len(path_parts) >= 2:\n",
    "        owner = path_parts[0]\n",
    "        repo = path_parts[1]\n",
    "        logging.info(f\"âœ… æˆåŠŸå¾ URL æå– owner:{owner}, repo:{repo} from {repo_url}\")\n",
    "        return owner, repo\n",
    "    elif len(path_parts) == 1:\n",
    "        owner = path_parts[0]\n",
    "        # å¦‚æœåªæœ‰ ownerï¼Œå˜—è©¦æŸ¥æ‰¾è©² owner ä¸‹æœ€æœ‰å¯èƒ½çš„ repo\n",
    "        logging.warning(f\"âš ï¸ åƒ…æ‰¾åˆ° GitHub owner: {owner} from {repo_url}ã€‚å˜—è©¦æŸ¥æ‰¾æœ€å¯èƒ½çš„å„²å­˜åº«...\")\n",
    "        return find_most_likely_repo(owner)\n",
    "    \n",
    "    logging.warning(f\"âŒ ç„¡æ³•å¾ URL æå– owner/repo (æ ¼å¼ä¸ç¬¦): {repo_url}\")\n",
    "    return None, None\n",
    "\n",
    "def find_most_likely_repo(owner):\n",
    "    \"\"\"å˜—è©¦å¾ GitHub çµ„ç¹”æˆ–ç”¨æˆ¶é é¢æ‰¾åˆ° commit æ•¸æœ€é«˜çš„å„²å­˜åº«ã€‚\"\"\"\n",
    "\n",
    "    # å˜—è©¦ä½œç‚ºç”¨æˆ¶æŸ¥æ‰¾\n",
    "    # GitHub API doesn't directly support sorting by commit count for repo listings.\n",
    "    # We'll fetch all repos and then individually check commit counts.\n",
    "    # For simplicity, we'll fetch a reasonable number of repos to check.\n",
    "    # A more robust solution might involve pagination if there are many repos.\n",
    "    url = f\"https://api.github.com/users/{owner}/repos?per_page=100\" # Fetch up to 100 repos\n",
    "    repos_data = fetch_with_retry(url, GITHUB_HEADERS)\n",
    "\n",
    "    if repos_data and isinstance(repos_data, list):\n",
    "        max_commits = -1\n",
    "        most_committed_repo = None\n",
    "\n",
    "        for repo in repos_data:\n",
    "            repo_name = repo['name']\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo_name}/commits?per_page=1\"\n",
    "            # We only need the 'Link' header to get the total number of commits\n",
    "            commits_response_headers = fetch_with_retry(commits_url, GITHUB_HEADERS, return_headers=True)\n",
    "\n",
    "            if commits_response_headers and 'Link' in commits_response_headers:\n",
    "                link_header = commits_response_headers['Link']\n",
    "                # Extract the last page number from the Link header\n",
    "                import re\n",
    "                last_page_match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "                if last_page_match:\n",
    "                    total_commits = int(last_page_match.group(1))\n",
    "                    if total_commits > max_commits:\n",
    "                        max_commits = total_commits\n",
    "                        most_committed_repo = repo_name\n",
    "            else:\n",
    "                # If no 'Link' header or other issue, try fetching the first page of commits\n",
    "                # and count them. This is less efficient but a fallback.\n",
    "                commits_data = fetch_with_retry(commits_url, GITHUB_HEADERS)\n",
    "                if commits_data and isinstance(commits_data, list):\n",
    "                    # If we can't get the total from 'Link', we'll assume the count on the first page\n",
    "                    # is the total if per_page=1 is used and we are only looking for one commit.\n",
    "                    # This is a simplification and might not be accurate for very large repos.\n",
    "                    # For a truly accurate count without 'Link' header, you'd need to paginate\n",
    "                    # through all commits.\n",
    "                    # For this purpose, we assume if the first commit is returned, there's at least one.\n",
    "                    if len(commits_data) > 0:\n",
    "                        # This fallback is imperfect as it doesn't give total count.\n",
    "                        # It primarily serves to check if there are *any* commits if 'Link' fails.\n",
    "                        # We'll skip this fallback for now as it makes the logic complex for true \"max commits\".\n",
    "                        pass\n",
    "\n",
    "        if most_committed_repo:\n",
    "            logging.info(f\"âœ… æ‰¾åˆ°ç”¨æˆ¶ {owner} commit æ•¸æœ€é«˜çš„å„²å­˜åº«: {most_committed_repo} (ç¸½æäº¤æ•¸: {max_commits})\")\n",
    "            return owner, most_committed_repo\n",
    "\n",
    "    # å˜—è©¦ä½œç‚ºçµ„ç¹”æŸ¥æ‰¾ (èˆ‡ç”¨æˆ¶é‚è¼¯ç›¸åŒ)\n",
    "    url = f\"https://api.github.com/orgs/{owner}/repos?per_page=100\" # Fetch up to 100 repos\n",
    "    repos_data = fetch_with_retry(url, GITHUB_HEADERS)\n",
    "\n",
    "    if repos_data and isinstance(repos_data, list):\n",
    "        max_commits = -1\n",
    "        most_committed_repo = None\n",
    "\n",
    "        for repo in repos_data:\n",
    "            repo_name = repo['name']\n",
    "            commits_url = f\"https://api.github.com/repos/{owner}/{repo_name}/commits?per_page=1\"\n",
    "            commits_response_headers = fetch_with_retry(commits_url, GITHUB_HEADERS, return_headers=True)\n",
    "\n",
    "            if commits_response_headers and 'Link' in commits_response_headers:\n",
    "                link_header = commits_response_headers['Link']\n",
    "                import re\n",
    "                last_page_match = re.search(r'page=(\\d+)>; rel=\"last\"', link_header)\n",
    "                if last_page_match:\n",
    "                    total_commits = int(last_page_match.group(1))\n",
    "                    if total_commits > max_commits:\n",
    "                        max_commits = total_commits\n",
    "                        most_committed_repo = repo_name\n",
    "\n",
    "        if most_committed_repo:\n",
    "            logging.info(f\"âœ… æ‰¾åˆ°çµ„ç¹” {owner} commit æ•¸æœ€é«˜çš„å„²å­˜åº«: {most_committed_repo} (ç¸½æäº¤æ•¸: {max_commits})\")\n",
    "            return owner, most_committed_repo\n",
    "\n",
    "    logging.warning(f\"âŒ ç„¡æ³•ç‚º owner: {owner} æ‰¾åˆ° commit æ•¸æœ€é«˜çš„å„²å­˜åº«ã€‚\")\n",
    "    return None, None\n",
    "\n",
    "def fetch_github_repo_stats(owner, repo):\n",
    "    \"\"\"å¾ GitHub API ç²å–å„²å­˜åº«çš„æäº¤æ´»å‹•ã€æ˜Ÿæ•¸å’Œ Fork æ•¸ã€‚\"\"\"\n",
    "    stats_url = f\"https://api.github.com/repos/{owner}/{repo}/stats/commit_activity\"\n",
    "    repo_info_url = f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "\n",
    "    logging.info(f\"å˜—è©¦ç²å–æäº¤æ´»å‹•: {stats_url} å»¶é²10ç§’é¿å…é™æµ\")\n",
    "    time.sleep(random.uniform(10, 15))\n",
    "    commit_activity = fetch_with_retry(stats_url, GITHUB_HEADERS)\n",
    "    \n",
    "    # è™•ç† 404 Not Found çš„æƒ…æ³\n",
    "    if commit_activity is None:\n",
    "        logging.warning(f\"âŒ ç„¡æ³•ç²å– {owner}/{repo} çš„æäº¤æ´»å‹•ï¼Œå¯èƒ½å„²å­˜åº«ä¸å­˜åœ¨æˆ–ç§æœ‰ã€‚\")\n",
    "        return None, None, None\n",
    "\n",
    "    # å°‡æ¯é€±çš„æäº¤æ•¸æå–å‡ºä¾†\n",
    "    commits_per_week = [week['total'] for week in commit_activity] if commit_activity else []\n",
    "    \n",
    "    logging.info(f\"å˜—è©¦ç²å–å„²å­˜åº«è³‡è¨Š: {repo_info_url}\")\n",
    "    repo_info = fetch_with_retry(repo_info_url, GITHUB_HEADERS)\n",
    "    \n",
    "    stars = repo_info.get('stargazers_count') if repo_info else None\n",
    "    forks = repo_info.get('forks_count') if repo_info else None\n",
    "\n",
    "    if not commits_per_week or stars is None or forks is None:\n",
    "        logging.warning(f\"âŒ æœªèƒ½ç²å– {owner}/{repo} çš„æ‰€æœ‰å¿…è¦çµ±è¨ˆæ•¸æ“šã€‚\")\n",
    "        return None, None, None\n",
    "\n",
    "    return commits_per_week, stars, forks\n",
    "\n",
    "def calc_commit_growth(commits_per_week):\n",
    "    \"\"\"è¨ˆç®— GitHub æäº¤æ´»å‹•çš„æˆé•·ç‡ã€‚\"\"\"\n",
    "    if len(commits_per_week) < 30: # è‡³å°‘éœ€è¦ 30 é€±æ•¸æ“šä¾†è¨ˆç®—å‰å¾Œ 15 é€±\n",
    "        logging.warning(f\"æ•¸æ“šä¸è¶³ (åªæœ‰ {len(commits_per_week)} é€±)ï¼Œç„¡æ³•è¨ˆç®—æˆé•·ç‡ã€‚\")\n",
    "        return None\n",
    "\n",
    "    recent_15w = sum(commits_per_week[-15:])\n",
    "    early_15w = sum(commits_per_week[-30:-15])\n",
    "\n",
    "    if early_15w == 0:\n",
    "        if recent_15w == 0:\n",
    "            return 0.0\n",
    "        else:\n",
    "            return float('inf') # é¿å…é™¤ä»¥é›¶ï¼Œè¡¨ç¤ºç„¡é™æˆé•·\n",
    "    \n",
    "    growth = ((recent_15w - early_15w) / early_15w) * 100\n",
    "    return growth\n",
    "\n",
    "def load_processed_symbols(csv_filename):\n",
    "    \"\"\"è¼‰å…¥å·²è™•ç†çš„å¹£ç¨®åˆ—è¡¨ã€‚\"\"\"\n",
    "    if os.path.exists(csv_filename):\n",
    "        df = pd.read_csv(csv_filename)\n",
    "        return set(df['symbol'].unique())\n",
    "    return set()\n",
    "\n",
    "def update_success_file(symbol):\n",
    "    \"\"\"æ›´æ–°æˆåŠŸè™•ç†çš„å¹£ç¨®åˆ—è¡¨ã€‚\"\"\"\n",
    "    with open(SUCCESS_FILE, 'a', encoding='utf-8') as f:\n",
    "        f.write(symbol + '\\n')\n",
    "\n",
    "def load_success_symbols():\n",
    "    \"\"\"è¼‰å…¥å·²æˆåŠŸè™•ç†çš„å¹£ç¨®ã€‚\"\"\"\n",
    "    if os.path.exists(SUCCESS_FILE):\n",
    "        with open(SUCCESS_FILE, 'r', encoding='utf-8') as f:\n",
    "            return {line.strip() for line in f if line.strip()}\n",
    "    return set()\n",
    "\n",
    "def main():\n",
    "    # ç¢ºä¿ CSV æª”æ¡ˆå­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨å‰‡å‰µå»ºå¸¶æœ‰æ¨™é ­çš„æª”æ¡ˆ\n",
    "    csv_filename = CSV_FILE\n",
    "    file_exists = os.path.exists(csv_filename)\n",
    "    if not file_exists:\n",
    "        # å®šç¾© CSV æª”æ¡ˆçš„æ¨™é ­\n",
    "        header = ['coin_id', 'symbol', 'repo', 'growth_%', 'recent_15w', 'early_15w', 'stars', 'forks']\n",
    "        df_empty = pd.DataFrame(columns=header)\n",
    "        df_empty.to_csv(csv_filename, index=False, encoding='utf-8')\n",
    "        logging.info(f\"å‰µå»ºæ–°çš„ CSV æª”æ¡ˆ: {csv_filename}\")\n",
    "\n",
    "    # è¼‰å…¥å·²è™•ç†éçš„å¹£ç¨®åˆ—è¡¨\n",
    "    processed_symbols_in_csv = load_processed_symbols(csv_filename)\n",
    "    logging.info(f\"å¾ {csv_filename} è¼‰å…¥ {len(processed_symbols_in_csv)} å€‹å·²è™•ç†ç¬¦è™Ÿã€‚\")\n",
    "\n",
    "    # ç²å–ç†±é–€å¹£ç¨®\n",
    "    top_symbols = get_top_symbols(limit=100)\n",
    "    if not top_symbols:\n",
    "        logging.error(\"æœªèƒ½ç²å–ç†±é–€å¹£ç¨®åˆ—è¡¨ï¼Œç¨‹å¼çµ‚æ­¢ã€‚\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"æ‰¾åˆ° {len(top_symbols)} å€‹ç†±é–€å¹£ç¨®ç¬¦è™Ÿ å‰äº”ç‚º:{top_symbols[:5]}\")\n",
    "\n",
    "    for symbol in tqdm(top_symbols, desc=\"è™•ç†å¹£ç¨®\"):\n",
    "        if symbol in processed_symbols_in_csv:\n",
    "            logging.info(f\"æ‰¾åˆ° {symbol} å­˜åœ¨csvä¸­è·³é\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"{symbol} ä¸å­˜åœ¨csvä¸­ é–‹å§‹è·‘åˆ†æ å˜—è©¦å–å¾—CoinGecko ID\")\n",
    "        coingecko_id = get_coingecko_id(symbol)\n",
    "        if not coingecko_id:\n",
    "            logging.warning(f\"âŒ ç„¡æ³•æ‰¾åˆ° {symbol} çš„ CoinGecko IDï¼Œè·³éã€‚\")\n",
    "            continue\n",
    "        logging.info(f\"æ‰¾åˆ° {symbol} çš„ CoinGecko ID:{coingecko_id}\")\n",
    "\n",
    "        # 1. å˜—è©¦å¾ CoinGecko ç²å– GitHub URL\n",
    "        repo_url = fetch_coingecko_github_repo_url(coingecko_id)\n",
    "        \n",
    "        # 2. å¦‚æœ CoinGecko æ²’æœ‰æä¾›ï¼Œå˜—è©¦ Google æœå°‹\n",
    "        if not repo_url and GOOGLE_API_KEY and GOOGLE_CSE_ID:\n",
    "            logging.info(f\"CoinGecko æœªæ‰¾åˆ° {coingecko_id} çš„ GitHub é€£çµï¼Œå˜—è©¦ Google æœå°‹...\")\n",
    "            # å˜—è©¦ç”¨ CoinGecko ID å’Œ å¹£ç¨®ç¬¦è™Ÿé€²è¡Œæœå°‹\n",
    "            repo_url = Google_Search_github_repo(f\"{coingecko_id} github\")\n",
    "            if not repo_url:\n",
    "                repo_url = Google_Search_github_repo(f\"{symbol} github\")\n",
    "            if not repo_url:\n",
    "                logging.warning(f\"âŒ ç„¡æ³•å¾ Google æœå°‹æ‰¾åˆ° {coingecko_id} æˆ– {symbol} çš„ GitHub repo é€£çµã€‚\")\n",
    "        \n",
    "        if not repo_url:\n",
    "            logging.warning(f\"âŒ ç„¡æ³•æ‰¾åˆ° {coingecko_id} çš„ GitHub repo é€£çµ\")\n",
    "            continue\n",
    "\n",
    "        logging.info(f\"æ‰¾åˆ° {coingecko_id} çš„ GitHub repo é€£çµ:{repo_url}\")\n",
    "\n",
    "        # 3. æå– owner å’Œ repo\n",
    "        owner, repo = extract_github_owner_repo(repo_url)\n",
    "        if not owner or not repo:\n",
    "            logging.warning(f\"âŒ ç„¡æ³•ç²å– {coingecko_id} çš„ GitHub owner/repo è³‡è¨Š repo_url:{repo_url}\")\n",
    "            continue\n",
    "        \n",
    "        # 4. æ ¹æ“š owner, repo æ‹¿åˆ° commits, stars, forks\n",
    "        commits, stars, forks = fetch_github_repo_stats(owner, repo)\n",
    "        if commits is None:\n",
    "            logging.warning(f\"æ‹¿å– {coingecko_id} çš„ commits, stars, forks å¤±æ•—\")\n",
    "            continue\n",
    "        logging.info(f\"æ‹¿å– {coingecko_id} çš„ commits, stars, forks æˆåŠŸ\")\n",
    "        \n",
    "        # 5. è¨ˆç®— commit æˆé•·\n",
    "        growth = calc_commit_growth(commits)\n",
    "        if growth is None:\n",
    "            logging.warning(f\"è¨ˆç®— growth å¤±æ•— coingecko_id:{coingecko_id}\")\n",
    "            continue\n",
    "        logging.info(f\"è¨ˆç®— growth æˆåŠŸ coingecko_id:{coingecko_id}\")\n",
    "        \n",
    "        # 6. æ–°å¢ä¸€ç­†ç´€éŒ„\n",
    "        new_record = [{\n",
    "            'coin_id': coingecko_id,\n",
    "            'symbol': symbol,\n",
    "            'repo': f'{owner}/{repo}',\n",
    "            'growth_%': growth,\n",
    "            'recent_15w': sum(commits[-15:]),\n",
    "            'early_15w': sum(commits[-30:-15]), # ä¿®æ­£ç‚ºæ­£ç¢ºçš„æ—©æœŸ15é€±\n",
    "            'stars': stars,\n",
    "            'forks': forks,\n",
    "        }]\n",
    "\n",
    "        if new_record:\n",
    "            new_df = pd.DataFrame(new_record)\n",
    "            new_df.to_csv(csv_filename, mode='a', header=False, index=False, encoding='utf-8') # header=False å› ç‚ºç¬¬ä¸€æ¬¡å·²å¯«å…¥\n",
    "            processed_symbols_in_csv.add(symbol) # å°‡æ–°è™•ç†çš„ç¬¦è™Ÿæ·»åŠ åˆ°å·²è™•ç†é›†åˆä¸­\n",
    "            time.sleep(random.uniform(1, 3)) # å¢åŠ éš¨æ©Ÿå»¶é²ï¼Œé¿å…éå¿«è«‹æ±‚\n",
    "            logging.info(f\"âœ… æˆåŠŸå°‡ {symbol} çš„æ•¸æ“šå¯«å…¥ {csv_filename}\")\n",
    "\n",
    "    logging.info(\"æ‰€æœ‰ç†±é–€å¹£ç¨®è™•ç†å®Œç•¢ã€‚\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vedanta",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
